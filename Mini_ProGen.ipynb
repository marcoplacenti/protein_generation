{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini-ProGen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LedgPH4DtZs"
      },
      "source": [
        "# Mini-ProGen Source Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgzfCV4TD1yV"
      },
      "source": [
        "##### Importing libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IQoxHYFCQH0"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm.std import tqdm\n",
        "import copy\n",
        "import difflib"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ6j0-mUD7qt"
      },
      "source": [
        "##### Defining Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvCZxT8vCyw5"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, padding_idx):\n",
        "      super().__init__()\n",
        "      self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
        "  def forward(self, x):\n",
        "      return self.embed(x)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zKhCCMwEEI_"
      },
      "source": [
        "##### Defining Position Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ZpLsXaC0jZ"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, embedding_size, max_seq_len=500, dropout = 0.1):\n",
        "      super().__init__()\n",
        "      self.embedding_size = embedding_size\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      # create constant 'pe' matrix with values dependant on \n",
        "      # pos and i\n",
        "      pe = torch.zeros(max_seq_len, embedding_size)\n",
        "      for pos in range(max_seq_len):\n",
        "          for i in range(0, embedding_size):\n",
        "              if i % 2 == 0:\n",
        "                  pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embedding_size)))\n",
        "              else:\n",
        "                  pe[pos, i] = math.cos(pos / (10000 ** ((2 * (i))/embedding_size)))\n",
        "      pe = pe.unsqueeze(0)\n",
        "      self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x, seq_len):\n",
        "      #x = x * math.sqrt(self.embedding_size)\n",
        "      #add constant to embedding\n",
        "      seq_len = x.size(1)\n",
        "      self.pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "      #if x.is_cuda:\n",
        "          #self.pe.cuda()\n",
        "      x = x + self.pe\n",
        "      return self.dropout(x)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbg5R2NCEIsF"
      },
      "source": [
        "##### Defining Decoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqwudEEOC2eW"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embedding_size, heads):\n",
        "      super().__init__()\n",
        "      self.norm_1 = Norm(embedding_size)\n",
        "      self.norm_2 = Norm(embedding_size)\n",
        "      \n",
        "      self.dropout_1 = nn.Dropout(0.1)\n",
        "      self.dropout_2 = nn.Dropout(0.1)\n",
        "      \n",
        "      self.attn_1 = MultiHeadAttention(heads, embedding_size, dropout=0.1)\n",
        "      self.ff = FeedForward(embedding_size, dropout=0.1)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "      x2 = self.norm_1(x)\n",
        "      x = x + self.dropout_1(self.attn_1(x2, x2, x2, mask))\n",
        "      x2 = self.norm_2(x)\n",
        "      x = x + self.dropout_2(self.ff(x2))\n",
        "      return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnuzw1eXEefk"
      },
      "source": [
        "##### Defining Norm Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzJQ_BP3C4me"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "  def __init__(self, embedding_size, eps = 1e-6):\n",
        "      super().__init__()\n",
        "  \n",
        "      self.size = embedding_size\n",
        "      \n",
        "      # create two learnable parameters to calibrate normalisation\n",
        "      self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "      self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "      \n",
        "      self.eps = eps\n",
        "  \n",
        "  def forward(self, x):\n",
        "      norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "      / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "      return norm"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "melw4YHWEh52"
      },
      "source": [
        "##### Defining Multi-Head Attention Layer and attention function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0OdPIvoC6jE"
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "      mask = mask.unsqueeze(1)\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "  scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "  if dropout is not None:\n",
        "      scores = dropout(scores)\n",
        "\n",
        "  output = torch.matmul(scores, v)\n",
        "  return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, heads, embedding_size, dropout = 0.1):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.embedding_size = embedding_size\n",
        "      self.d_k = embedding_size // heads\n",
        "      self.h = heads\n",
        "      \n",
        "      self.q_linear = nn.Linear(embedding_size, embedding_size)\n",
        "      self.v_linear = nn.Linear(embedding_size, embedding_size)\n",
        "      self.k_linear = nn.Linear(embedding_size, embedding_size)\n",
        "      \n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.out = nn.Linear(embedding_size, embedding_size)\n",
        "  \n",
        "  def forward(self, q, k, v, mask=None):\n",
        "      \n",
        "      bs = q.size(0)\n",
        "      \n",
        "      # perform linear operation and split into N heads\n",
        "      k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "      q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "      v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "      \n",
        "      # transpose to get dimensions bs * N * sl * embedding_size\n",
        "      k = k.transpose(1,2)\n",
        "      q = q.transpose(1,2)\n",
        "      v = v.transpose(1,2)\n",
        "      \n",
        "      # calculate attention using function we will define next\n",
        "      scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "      # concatenate heads and put through final linear layer\n",
        "      concat = scores.transpose(1,2).contiguous().view(bs, -1, self.embedding_size)\n",
        "      output = self.out(concat)\n",
        "  \n",
        "      return output"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcC6_ggPEnTO"
      },
      "source": [
        "##### Defining Feed Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Grtfj8C-Yc"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedding_size, d_ff=2048, dropout = 0.1):\n",
        "      super().__init__() \n",
        "      # We set d_ff as a default to 2048\n",
        "      self.linear_1 = nn.Linear(embedding_size, d_ff)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.linear_2 = nn.Linear(d_ff, embedding_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      x = self.dropout(F.relu(self.linear_1(x)))\n",
        "      x = self.linear_2(x)\n",
        "      return x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x14PNSRAE4lE"
      },
      "source": [
        "##### Defining Stacked Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUUVFbG5C_0H"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, embedding_size, heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.norm_1 = Norm(embedding_size)\n",
        "    self.norm_2 = Norm(embedding_size)\n",
        "    \n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "    \n",
        "    self.attn_1 = MultiHeadAttention(heads, embedding_size, dropout=0.1)\n",
        "    self.ff = FeedForward(embedding_size, dropout=0.1)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    x2 = self.norm_1(x)\n",
        "    x = x + self.dropout_1(self.attn_1(x2, x2, x2, mask))\n",
        "    x2 = self.norm_2(x)\n",
        "    x = x + self.dropout_2(self.ff(x2))\n",
        "    return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOwagw4xFHt1"
      },
      "source": [
        "##### Putting the whole architecture together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kG6REg5DDga"
      },
      "source": [
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class ProGen(nn.Module):\n",
        "  def __init__(self, vocab_size, tensor_length, embeddings_size, heads, padding_idx, number_of_layers):\n",
        "      super().__init__()\n",
        "      self.number_of_layers = number_of_layers\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedder = Embedder(vocab_size, embeddings_size, padding_idx)\n",
        "      self.pe = PositionalEncoder(embeddings_size, max_seq_len=tensor_length, dropout=0.1)\n",
        "      self.layers = get_clones(DecoderLayer(embeddings_size, heads, 0.1), number_of_layers)\n",
        "      self.toprobs = nn.Linear(embeddings_size, vocab_size)\n",
        "\n",
        "  \n",
        "  def forward(self, seq, mask=None):\n",
        "      x = self.embedder(seq)\n",
        "      x = self.pe(x, seq.shape[0])\n",
        "      for i in range(self.number_of_layers):\n",
        "          x = self.layers[i](x, mask)\n",
        "      #x = self.decoder(x, mask)\n",
        "      \n",
        "      batch, seq_len, emb_size = x.shape\n",
        "\n",
        "      x = x.view(batch * seq_len, emb_size)   # [ batch*seq_len,  emb_dim ]\n",
        "      x = self.toprobs(x)                                 # [ batch*seq_len,  num_tokens ]\n",
        "      x = x.view(batch, self.vocab_size, seq_len) # [ batch, num_tokens, seq_len ]\n",
        "\n",
        "      return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjhvOLitFaDE"
      },
      "source": [
        "##### Function generating masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdmWLkPBDdis"
      },
      "source": [
        "def create_mask(s, pad_id): \n",
        "  pad_idx = np.argwhere(s == pad_id)\n",
        "\n",
        "  dim = s.shape[0]\n",
        "  s_mask = np.full((dim, dim), True)\n",
        "  for i in range(dim):\n",
        "      s_mask[i][i+1:] = False\n",
        "      curr_seq = s[:i+1]\n",
        "      s_mask[i][pad_idx] = False\n",
        "\n",
        "  return s_mask"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLtxcH12Ht8Q"
      },
      "source": [
        "##### Reading and processing data to get it ready for the Mini-ProGen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNd0wd58DkQG"
      },
      "source": [
        "def get_data(max_length):\n",
        "  data = pd.read_csv(\"dataset.csv\")\n",
        "  data = data.replace(np.nan, '<PAD>', regex=True)\n",
        "  data = data[data[\"Sequence\"].map(len) <= max_length]\n",
        "\n",
        "  vocab = set()\n",
        "  for col in data.columns:\n",
        "      if col != \"Sequence\":\n",
        "          vocab.update(data[col])\n",
        "\n",
        "  seq_len = []\n",
        "  max_seq = 0\n",
        "  for seq in data[\"Sequence\"]:\n",
        "      seq = [s for s in seq]\n",
        "      seq_len.append(len(seq))\n",
        "      if len(seq) > max_seq:\n",
        "          max_seq = len(seq)\n",
        "      vocab.update(seq)\n",
        "\n",
        "  #vocab.update([\"<EOT>\"])\n",
        "  vocab.update([\"<PAD>\"])\n",
        "  vocab.update([\"<EOS>\"])\n",
        "\n",
        "  elements = set()\n",
        "  for col in data.columns[1:-9]:\n",
        "      elements.update(data[col].unique())\n",
        "\n",
        "  elem_to_col = {}\n",
        "  for i, element in enumerate(elements):\n",
        "      data[element] = \"<PAD>\"\n",
        "\n",
        "  for row in data.iterrows():\n",
        "      values = np.unique(row[1][data.columns[1:-9-len(elements)]].values)\n",
        "      values = np.delete(values, np.argwhere(values==\"<PAD>\"))\n",
        "      for element in elements:\n",
        "          if element in values:\n",
        "              row[1][element] = element\n",
        "\n",
        "  data.drop(data.columns[1:-9-len(elements)], axis=1, inplace=True)\n",
        "  \n",
        "  seq = data.pop(\"Sequence\")\n",
        "  data[\"Sequence\"] = seq\n",
        "\n",
        "  data.to_csv(\"dataset_processed.csv\", index=False)\n",
        "  \n",
        "\n",
        "  return data, vocab, max_seq"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wl-wnOuDm0E"
      },
      "source": [
        "def process_data(data, vocab, max_seq):\n",
        "  token_to_id, id_to_token = {}, {}\n",
        "  \n",
        "  token_to_id[\"<PAD>\"] = 0\n",
        "  id_to_token[0] = \"<PAD>\"\n",
        "\n",
        "  token_to_id[\"<EOS>\"] = 1\n",
        "  id_to_token[1] = \"<EOS>\"\n",
        "\n",
        "  for i, token in enumerate(vocab):\n",
        "      id = len(token_to_id.keys())\n",
        "      if token != \"<PAD>\" and token != \"<EOS>\":\n",
        "          token_to_id[token] = id\n",
        "          id_to_token[id] = token\n",
        "          id += 1\n",
        "\n",
        "  seq = []\n",
        "  \n",
        "  for record in data.values:\n",
        "      tags = record[:-1]\n",
        "      sequence = record[-1]\n",
        "\n",
        "      encoded_record = [token_to_id[tag] for tag in tags]\n",
        "\n",
        "      for char in sequence:\n",
        "          encoded_record.append(token_to_id[char])\n",
        "      encoded_record.append(token_to_id[\"<EOS>\"])\n",
        "      \n",
        "      if len(sequence) < max_seq:\n",
        "          for i in range(max_seq-len(sequence)):\n",
        "              encoded_record.append(token_to_id[\"<PAD>\"])\n",
        "\n",
        "      seq.append(encoded_record)\n",
        "\n",
        "      \"\"\"\n",
        "      enc_rec_no_tags = []#[token_to_id[\"<PAD>\"] for tag in tags]\n",
        "      for char in sequence:\n",
        "          enc_rec_no_tags.append(token_to_id[char])\n",
        "      enc_rec_no_tags.append(token_to_id[\"<EOS>\"])\n",
        "      #if len(sequence) < max_seq:\n",
        "      for _ in range(max_seq-len(sequence)):\n",
        "          enc_rec_no_tags.append(token_to_id[\"<PAD>\"])\n",
        "      seq.append(enc_rec_no_tags)\n",
        "      \"\"\"\n",
        "\n",
        "  return np.array(seq), token_to_id, id_to_token"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM24rU3VH3vL"
      },
      "source": [
        "##### Functions used for generating next token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4df68PHDo29"
      },
      "source": [
        "def sample_categorical(lnprobs, temperature=1.0):\n",
        "  \"\"\"\n",
        "  Sample an element from a categorical distribution\n",
        "  :param lnprobs: Outcome log-probabilities\n",
        "  :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
        "      0.0 returns the maximum probability element.\n",
        "  :return: The index of the sampled element.\n",
        "  \"\"\"\n",
        "  if temperature == 0.0:\n",
        "      return lnprobs.argmax()\n",
        "  p = F.softmax(lnprobs / temperature, dim=0)\n",
        "  return dist.Categorical(p).sample()\n",
        "\n",
        "def sample_sentence(model, query, max_len = 140, temperature=1):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  for _ in range(max_len - query.shape[0]):\n",
        "      query_ = torch.zeros(max_len).to(torch.long)\n",
        "      query_[:len(query)] = query\n",
        "      output = model(query_.unsqueeze(0).to(device))\n",
        "      next_char_idx = sample_categorical(output[0, :, len(query) - 1], 0) #0.5\n",
        "      query = query.tolist()\n",
        "      query.append(int(next_char_idx))\n",
        "      query = torch.from_numpy(np.array(query))\n",
        "      if int(next_char_idx) < 2:\n",
        "          break\n",
        "  return query\n",
        "\n",
        "\n",
        "def make_sequence_from_tokens(ids, id_to_token):\n",
        "  sequence = map(lambda x: id_to_token[x], ids.tolist())\n",
        "  return \" \".join(list(sequence))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf7Ls_GzDq8G"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  data, vocab, max_seq = get_data(max_length=300)\n",
        "  seq, token_to_id, id_to_token = process_data(data, vocab, max_seq)\n",
        "  seq = torch.from_numpy(seq).to(device)\n",
        "\n",
        "  x = seq\n",
        "  y = torch.hstack((x[:,1:], torch.zeros(x.shape[0], 1, dtype=torch.int32))).to(device)\n",
        "  \n",
        "  mask = []\n",
        "  for i, s in enumerate(tqdm(x, desc=\"Creating masks\")):\n",
        "      mask.append(create_mask(s, token_to_id[\"<PAD>\"]))\n",
        "  \n",
        "  mask = torch.from_numpy(np.array(mask)).to(device)\n",
        "  \n",
        "  embedding_sizes = [32, 64, 128, 512]\n",
        "  heads = [1, 2, 4, 8]\n",
        "  no_stacked_layers = [3, 4, 5, 6]\n",
        "\n",
        "  metrics = open('metrics.csv', 'w')\n",
        "  metrics.write(\"EMBEDDING_SIZE, HEADS, NUMBER OF LAYERS, EPOCH, TRAIN_LOSS, TRAIN_PERP, TEST_LOSS, TEST_PERP\\n\")\n",
        "  generations = open(\"generations.csv\", \"w\")\n",
        "  generations.write(\"EMBEDDING_SIZE, HEADS, NUMBER_OF_LAYERS, EPOCH, AVG_SIM, SAMPLE\\n\")\n",
        "\n",
        "  for i in range(len(heads)):\n",
        "      print(\"-----------------------------------------\")\n",
        "      print(f\"RUNNING CONFIGURATION {i+1}/{len(heads)}...\")\n",
        "      embedding_size = embedding_sizes[i]\n",
        "      head = heads[i]\n",
        "      number_of_layers = no_stacked_layers[i]\n",
        "\n",
        "      def get_n_params(model):\n",
        "          pp=0\n",
        "          for p in list(model.parameters()):\n",
        "              nn=1\n",
        "              for s in list(p.size()):\n",
        "                  nn = nn*s\n",
        "              pp += nn\n",
        "          return pp\n",
        "\n",
        "      # embedding_size needs to be divisible by heads\n",
        "      model = ProGen(vocab_size=len(vocab), embeddings_size=embedding_size, tensor_length=seq.shape[0], heads=head, padding_idx=token_to_id[\"<PAD>\"], number_of_layers=number_of_layers).to(device)\n",
        "\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "      scheduler = MultiStepLR(optimizer, milestones=[60, 100, 150], gamma=0.1)\n",
        "\n",
        "      batch_size = 32\n",
        "      for epoch in tqdm(range(100), desc='Running epochs...'):\n",
        "          batch_idx = random.sample(range(seq.shape[0]), batch_size*2)\n",
        "          train_idx = batch_idx[:batch_size]\n",
        "          test_idx = batch_idx[batch_size:]\n",
        "\n",
        "          model.train()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          train_input = x[train_idx]\n",
        "          train_masks = mask[train_idx]\n",
        "          train_y = y[train_idx]\n",
        "\n",
        "          preds = model(train_input, train_masks)\n",
        "\n",
        "          loss = F.nll_loss(preds, train_y, reduction='mean', ignore_index=0)\n",
        "          nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "          train_total_loss = loss.item()\n",
        "          \n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "          model.eval()\n",
        "          \n",
        "          test_input = x[test_idx]\n",
        "          test_masks = mask[test_idx]\n",
        "          test_y = y[test_idx]\n",
        "\n",
        "          preds = model(test_input, test_masks)\n",
        "          loss = F.nll_loss(preds, test_y, reduction='mean', ignore_index=0)\n",
        "          \n",
        "          test_total_loss = loss.item()\n",
        "\n",
        "          metrics.write(\"%s, %s, %s, %s, %s, %s, %s, %s\\n\" % \\\n",
        "              (embedding_size, head, number_of_layers, epoch, \\\n",
        "                  str(round(train_total_loss,3)), str(round(math.exp(train_total_loss),3)), \\\n",
        "                  str(round(test_total_loss,3)), str(round(math.exp(test_total_loss),3))))\n",
        "      \n",
        "          \n",
        "          if epoch % 10 == 0:# and epoch != 0:\n",
        "              avg_sim = 0\n",
        "              print(make_sequence_from_tokens(x[27], id_to_token))\n",
        "              cond = torch.from_numpy(np.array(x[27]))[:169+10]\n",
        "              generated_seq = sample_sentence(model, cond, max_len = 291, temperature = 0)[169+10:]\n",
        "              sampled = make_sequence_from_tokens(generated_seq, id_to_token)\n",
        "              print(sampled)\n",
        "          \n",
        "              avg_sim += difflib.SequenceMatcher(None, generated_seq, torch.from_numpy(np.array(x[0]))[169+10:]).ratio()\n",
        "              generations.write(\"%s, %s, %s, %s, %s, %s\\n\" % (embedding_size, head, number_of_layers, epoch, avg_sim, sampled))\n",
        "\n",
        "\n",
        "      print(\"-----------------------------------------\")\n",
        "      print()\n",
        "      \n",
        "      metrics.close()\n",
        "      generations.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCJMzVWZGWaz"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}